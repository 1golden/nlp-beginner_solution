{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实验目的：实现基于CNN、RNN的文本分类\n",
    "\n",
    "实验内容：\n",
    "1）词嵌入初始化方式：随机embedding、加载glove\n",
    "2）CNN/RNN的特征抽取\n",
    "3）Dropout\n",
    "\n",
    "\n",
    "参考：\n",
    "https://arxiv.org/abs/1408.5882\n",
    "https://github.com/yokusama/CNN_Sentence_Classification\n",
    "https://torchtext.readthedocs.io/en/latest/\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\nlp_beginer_solution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "print(os.getcwd())\n",
    "\n",
    "dir_all_data='data\\\\task2_all_data.tsv'\n",
    "\n",
    "BATCH_SIZE=10\n",
    "\n",
    "cpu=True   #True   False \n",
    "if cpu :\n",
    "    USE_CUDA = False\n",
    "    DEVICE = torch.device('cpu')\n",
    "else:\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件中读取数据\n",
    "data_all=pd.read_csv(dir_all_data,sep='\\t')\n",
    "#print(all_data.shape)    #(156060, 4)\n",
    "#print(all_data.keys())   #['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']\n",
    "idx =np.arange(data_all.shape[0])\n",
    "#print(data_all.head())\n",
    "#print(type(idx))   #<class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle，划分验证集、测试集,并保存\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "#print(idx)\n",
    "np.random.shuffle(idx)  \n",
    "#print(idx)\n",
    "\n",
    "train_size=int(len(idx) * 0.6)\n",
    "test_size =int(len(idx) * 0.8)\n",
    "\n",
    "data_all.iloc[idx[:train_size], :].to_csv('data/task2_train.csv',index=False)\n",
    "data_all.iloc[idx[train_size:test_size], :].to_csv(\"data/task2_test.csv\", index=False)\n",
    "data_all.iloc[idx[test_size:], :].to_csv(\"data/task2_dev.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torchtext采用声明式方法加载数据\n",
    "from torchtext import data\n",
    "PAD_TOKEN='<pad>'\n",
    "TEXT = data.Field(sequential=True,batch_first=True, lower=True, pad_token=PAD_TOKEN)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, unk_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "\n",
    "datafields = [(\"PhraseId\", None), # 不需要的filed设置为None\n",
    "              (\"SentenceId\", None),\n",
    "              ('Phrase', TEXT),\n",
    "              ('Sentiment', LABEL)]\n",
    "train_data = data.TabularDataset(path='data/task2_train.csv', format='csv',\n",
    "                                fields=datafields)\n",
    "dev_data  = data.TabularDataset(path='data/task2_dev.csv', format='csv',\n",
    "                                fields=datafields)\n",
    "test_data = data.TabularDataset(path='data/task2_test.csv', format='csv',\n",
    "                                fields=datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词典，字符映射到embedding\n",
    "#TEXT.vocab.vectors 就是词向量\n",
    "TEXT.build_vocab(train_data,  vectors= 'glove.6B.50d',   #可以提前下载好\n",
    "                 unk_init= lambda x:torch.nn.init.uniform_(x, a=-0.25, b=0.25))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "\n",
    "#得到索引，PAD_TOKEN='<pad>'\n",
    "PAD_INDEX = TEXT.vocab.stoi[PAD_TOKEN]\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建迭代器\n",
    "train_iterator = data.BucketIterator(train_data, batch_size=BATCH_SIZE, \n",
    "                                     train=True, shuffle=True,device=DEVICE)\n",
    "\n",
    "dev_iterator = data.Iterator(dev_data, batch_size=len(dev_data), train=False,\n",
    "                         sort=False, device=DEVICE)\n",
    "\n",
    "test_iterator = data.Iterator(test_data, batch_size=len(test_data), train=False,\n",
    "                          sort=False, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#部分参数设置\n",
    "embedding_choice='glove'   #  'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "embedding_dim =50\n",
    "dropout_p=0.5\n",
    "filters_num=100\n",
    "\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "print(vocab_size,label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding_choice=embedding_choice\n",
    "        \n",
    "        if self.embedding_choice==  'rand':\n",
    "            self.embedding=nn.Embedding(num_embeddings,embedding_dim)\n",
    "        if self.embedding_choice==  'glove':\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim, \n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "            \n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(3, embedding_dim), padding=(2,0))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(4, embedding_dim), padding=(3,0))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(5, embedding_dim), padding=(4,0))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(filters_num * 3, label_num)\n",
    "        \n",
    "    def forward(self,x):      # (Batch_size, Length) \n",
    "        x=self.embedding(x).unsqueeze(1)      #(Batch_size, Length, Dimention) \n",
    "                                       #(Batch_size, 1, Length, Dimention) \n",
    "        \n",
    "        x1 = F.relu(self.conv1(x)).squeeze(3)    #(Batch_size, filters_num, length+padding, 1) \n",
    "                                          #(Batch_size, filters_num, length+padding) \n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)  #(Batch_size, filters_num, 1)\n",
    "                                               #(Batch_size, filters_num) \n",
    "         \n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)  \n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)      \n",
    "        \n",
    "        x3 = F.relu(self.conv3(x)).squeeze(3)  \n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)      \n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), dim=1)  #(Batch_size, filters_num *3 )\n",
    "        x = self.dropout(x)      #(Batch_size, filters_num *3 )\n",
    "        out = self.fc(x)       #(Batch_size, label_num  )\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型\n",
    "\n",
    "model=CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()   #损失函数\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_1.068%:  Training average Loss: 1.396368, Training accuracy: 0.005265\n",
      "Epoch 0_2.136%:  Training average Loss: 1.339209, Training accuracy: 0.010712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-88aa435ab721>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "epoch=100\n",
    "best_accuracy=0.0\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        #print(steps)\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "        \n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)    #[batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "                   .view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f, Training accuracy: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps, total_correct/total_data_num))  \n",
    "\n",
    "    #验证\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(dev_iterator.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in dev_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "        \n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  \n",
    "        \n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num \n",
    "            torch.save(model,'model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            #torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average Loss: 0.956620, Test accuracy: 0.213911，Total time: 38.816360\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "PATH='model_dict/model_glove/epoch_8'\n",
    "model = torch.load(PATH)\n",
    "\n",
    "total_loss=0.0\n",
    "accuracy=0.0\n",
    "total_correct=0.0\n",
    "total_data_num = len(train_iterator.dataset)\n",
    "steps = 0.0    \n",
    "start_time=time.time()\n",
    "for batch in test_iterator:\n",
    "    steps+=1\n",
    "    batch_text=batch.Phrase\n",
    "    batch_label=batch.Sentiment\n",
    "    out=model(batch_text)\n",
    "    loss = criterion(out, batch_label)\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "    total_correct = total_correct + correct.item()\n",
    "\n",
    "print(\"Test average Loss: %f, Test accuracy: %f，Total time: %f\"\n",
    "  %(total_loss/steps, total_correct/total_data_num,time.time()-start_time) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
