{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n实验目的：实现基于CNN、RNN的文本分类\\n\\n实验内容：\\n1）词嵌入初始化方式：随机embedding、加载glove\\n2）CNN/RNN的特征抽取\\n3）Dropout\\n\\n\\n参考：\\nhttps://arxiv.org/abs/1408.5882\\nhttps://github.com/yokusama/CNN_Sentence_Classification\\nhttps://torchtext.readthedocs.io/en/latest/\\nhttp://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\\nhttps://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\\nhttps://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "实验目的：实现基于CNN、RNN的文本分类\n",
    "\n",
    "实验内容：\n",
    "1）词嵌入初始化方式：随机embedding、加载glove\n",
    "2）CNN/RNN的特征抽取\n",
    "3）Dropout\n",
    "\n",
    "\n",
    "参考：\n",
    "https://arxiv.org/abs/1408.5882\n",
    "https://github.com/yokusama/CNN_Sentence_Classification\n",
    "https://torchtext.readthedocs.io/en/latest/\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\W_project\\ML_and_DL\\nlp-beginner_solution\\Task2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "dir_all_data = \"data\\\\task2_all_data.tsv\"\n",
    "\n",
    "# 超参数设置\n",
    "BATCH_SIZE = 10\n",
    "cpu = True  # True   False\n",
    "if cpu:\n",
    "    USE_CUDA = False\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "else:\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读取数据\n",
    "data_all = pd.read_csv(dir_all_data, sep=\"\\t\")\n",
    "# print(all_data.shape)    #(156060, 4)\n",
    "# print(all_data.keys())   #['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']\n",
    "idx = np.arange(data_all.shape[0])\n",
    "# print(data_all.head())\n",
    "# print(type(idx))   #<class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle、划分验证集、测试集,并保存\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "# print(idx)\n",
    "np.random.shuffle(idx)\n",
    "# print(idx)\n",
    "\n",
    "train_size = int(len(idx) * 0.6)\n",
    "test_size = int(len(idx) * 0.8)\n",
    "\n",
    "data_all.iloc[idx[:train_size], :].to_csv(\"data/task2_train.csv\", index=False)\n",
    "data_all.iloc[idx[train_size:test_size], :].to_csv(\"data/task2_test.csv\", index=False)\n",
    "data_all.iloc[idx[test_size:], :].to_csv(\"data/task2_dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Torchtext采用声明式方法加载数据\n",
    "from torchtext import data\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True, pad_token=PAD_TOKEN)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x28449429210>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "datafields = [\n",
    "    (\"PhraseId\", None),  # 不需要的filed设置为None\n",
    "    (\"SentenceId\", None),\n",
    "    (\"Phrase\", TEXT),\n",
    "    (\"Sentiment\", LABEL),\n",
    "]\n",
    "train_data = data.TabularDataset(\n",
    "    path=\"data/task2_train.csv\", format=\"csv\", fields=datafields\n",
    ")\n",
    "dev_data = data.TabularDataset(\n",
    "    path=\"data/task2_dev.csv\", format=\"csv\", fields=datafields\n",
    ")\n",
    "test_data = data.TabularDataset(\n",
    "    path=\"data/task2_test.csv\", format=\"csv\", fields=datafields\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 400000/400001 [00:14<00:00, 27254.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 构建词典，字符映射到embedding\n",
    "# TEXT.vocab.vectors 就是词向量\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "glove_vectors = Vectors(\n",
    "    \"F:\\\\W_project\\\\ML_and_DL\\\\nlp-beginner_solution\\\\txt_classify\\\\glove.6B\\\\glove.6B.50d.txt\",\n",
    "    cache=\"nlp-beginner_solution\\\\txt_classify\",\n",
    ")\n",
    "TEXT.build_vocab(\n",
    "    train_data,\n",
    "    vectors=glove_vectors,\n",
    "    unk_init=lambda x: torch.nn.init.uniform_(x, a=-0.25, b=0.25),\n",
    ")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "\n",
    "# 得到索引，PAD_TOKEN='<pad>'\n",
    "PAD_INDEX = TEXT.vocab.stoi[PAD_TOKEN]\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x2844c129630>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建迭代器\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data, batch_size=BATCH_SIZE, train=True, shuffle=True, device=DEVICE\n",
    ")\n",
    "\n",
    "dev_iterator = data.Iterator(\n",
    "    dev_data, batch_size=len(dev_data), train=False, sort=False, device=DEVICE\n",
    ")\n",
    "\n",
    "test_iterator = data.Iterator(\n",
    "    test_data, batch_size=len(test_data), train=False, sort=False, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16473 6\n"
     ]
    }
   ],
   "source": [
    "# 部分参数设置\n",
    "embedding_choice = \"glove\"  #  'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "embedding_dim = 50\n",
    "dropout_p = 0.5\n",
    "filters_num = 100\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_num = len(LABEL.vocab)\n",
    "print(vocab_size, label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding_choice = embedding_choice\n",
    "\n",
    "        if self.embedding_choice == \"rand\":\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        if self.embedding_choice == \"glove\":\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings, embedding_dim, padding_idx=PAD_INDEX\n",
    "            ).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=filters_num,  # 卷积产生的通道\n",
    "            kernel_size=(3, embedding_dim),\n",
    "            padding=(2, 0),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=filters_num,  # 卷积产生的通道\n",
    "            kernel_size=(4, embedding_dim),\n",
    "            padding=(3, 0),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=filters_num,  # 卷积产生的通道\n",
    "            kernel_size=(5, embedding_dim),\n",
    "            padding=(4, 0),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc = nn.Linear(filters_num * 3, label_num)\n",
    "\n",
    "    def forward(self, x):  # (Batch_size, Length)\n",
    "        x = self.embedding(x).unsqueeze(1)  # (Batch_size, Length, Dimention)\n",
    "        # (Batch_size, 1, Length, Dimention)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x)).squeeze(\n",
    "            3\n",
    "        )  # (Batch_size, filters_num, length+padding, 1)\n",
    "        # (Batch_size, filters_num, length+padding)\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)  # (Batch_size, filters_num, 1)\n",
    "        # (Batch_size, filters_num)\n",
    "\n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "\n",
    "        x3 = F.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)  # (Batch_size, filters_num *3 )\n",
    "        x = self.dropout(x)  # (Batch_size, filters_num *3 )\n",
    "        out = self.fc(x)  # (Batch_size, label_num  )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "\n",
    "model = CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_1.068%:  Training average Loss: 1.370474\n",
      "Epoch 0_2.136%:  Training average Loss: 1.336910\n",
      "Epoch 0_3.204%:  Training average Loss: 1.293310\n",
      "Epoch 0_4.272%:  Training average Loss: 1.266158\n",
      "Epoch 0_5.340%:  Training average Loss: 1.234342\n",
      "Epoch 0_6.408%:  Training average Loss: 1.213062\n",
      "Epoch 0_7.476%:  Training average Loss: 1.195165\n",
      "Epoch 0_8.544%:  Training average Loss: 1.182784\n",
      "Epoch 0_9.612%:  Training average Loss: 1.174780\n",
      "Epoch 0_10.680%:  Training average Loss: 1.166213\n",
      "Epoch 0_11.747%:  Training average Loss: 1.162433\n",
      "Epoch 0_12.815%:  Training average Loss: 1.154645\n",
      "Epoch 0_13.883%:  Training average Loss: 1.148871\n",
      "Epoch 0_14.951%:  Training average Loss: 1.144656\n",
      "Epoch 0_16.019%:  Training average Loss: 1.142490\n",
      "Epoch 0_17.087%:  Training average Loss: 1.139114\n",
      "Epoch 0_18.155%:  Training average Loss: 1.135034\n",
      "Epoch 0_19.223%:  Training average Loss: 1.132984\n",
      "Epoch 0_20.291%:  Training average Loss: 1.128309\n",
      "Epoch 0_21.359%:  Training average Loss: 1.124403\n",
      "Epoch 0_22.427%:  Training average Loss: 1.120930\n",
      "Epoch 0_23.495%:  Training average Loss: 1.121096\n",
      "Epoch 0_24.563%:  Training average Loss: 1.120862\n",
      "Epoch 0_25.631%:  Training average Loss: 1.120872\n",
      "Epoch 0_26.699%:  Training average Loss: 1.119511\n",
      "Epoch 0_27.767%:  Training average Loss: 1.118653\n",
      "Epoch 0_28.835%:  Training average Loss: 1.116369\n",
      "Epoch 0_29.903%:  Training average Loss: 1.114587\n",
      "Epoch 0_30.971%:  Training average Loss: 1.112961\n",
      "Epoch 0_32.039%:  Training average Loss: 1.111661\n",
      "Epoch 0_33.107%:  Training average Loss: 1.108384\n",
      "Epoch 0_34.175%:  Training average Loss: 1.106956\n",
      "Epoch 0_35.242%:  Training average Loss: 1.104784\n",
      "Epoch 0_36.310%:  Training average Loss: 1.103685\n",
      "Epoch 0_37.378%:  Training average Loss: 1.102699\n",
      "Epoch 0_38.446%:  Training average Loss: 1.102439\n",
      "Epoch 0_39.514%:  Training average Loss: 1.101278\n",
      "Epoch 0_40.582%:  Training average Loss: 1.100477\n",
      "Epoch 0_41.650%:  Training average Loss: 1.099773\n",
      "Epoch 0_42.718%:  Training average Loss: 1.099030\n",
      "Epoch 0_43.786%:  Training average Loss: 1.098312\n",
      "Epoch 0_44.854%:  Training average Loss: 1.098174\n",
      "Epoch 0_45.922%:  Training average Loss: 1.098027\n",
      "Epoch 0_46.990%:  Training average Loss: 1.097207\n",
      "Epoch 0_48.058%:  Training average Loss: 1.095402\n",
      "Epoch 0_49.126%:  Training average Loss: 1.093638\n",
      "Epoch 0_50.194%:  Training average Loss: 1.092237\n",
      "Epoch 0_51.262%:  Training average Loss: 1.091687\n",
      "Epoch 0_52.330%:  Training average Loss: 1.090308\n",
      "Epoch 0_53.398%:  Training average Loss: 1.088646\n",
      "Epoch 0_54.466%:  Training average Loss: 1.087913\n",
      "Epoch 0_55.534%:  Training average Loss: 1.087545\n",
      "Epoch 0_56.602%:  Training average Loss: 1.087346\n",
      "Epoch 0_57.670%:  Training average Loss: 1.087254\n",
      "Epoch 0_58.737%:  Training average Loss: 1.086952\n",
      "Epoch 0_59.805%:  Training average Loss: 1.086681\n",
      "Epoch 0_60.873%:  Training average Loss: 1.086178\n",
      "Epoch 0_61.941%:  Training average Loss: 1.086154\n",
      "Epoch 0_63.009%:  Training average Loss: 1.084511\n",
      "Epoch 0_64.077%:  Training average Loss: 1.084008\n",
      "Epoch 0_65.145%:  Training average Loss: 1.083006\n",
      "Epoch 0_66.213%:  Training average Loss: 1.082456\n",
      "Epoch 0_67.281%:  Training average Loss: 1.081557\n",
      "Epoch 0_68.349%:  Training average Loss: 1.081612\n",
      "Epoch 0_69.417%:  Training average Loss: 1.080095\n",
      "Epoch 0_70.485%:  Training average Loss: 1.079049\n",
      "Epoch 0_71.553%:  Training average Loss: 1.078415\n",
      "Epoch 0_72.621%:  Training average Loss: 1.078349\n",
      "Epoch 0_73.689%:  Training average Loss: 1.077817\n",
      "Epoch 0_74.757%:  Training average Loss: 1.076872\n",
      "Epoch 0_75.825%:  Training average Loss: 1.075648\n",
      "Epoch 0_76.893%:  Training average Loss: 1.075493\n",
      "Epoch 0_77.961%:  Training average Loss: 1.075159\n",
      "Epoch 0_79.029%:  Training average Loss: 1.074346\n",
      "Epoch 0_80.097%:  Training average Loss: 1.073912\n",
      "Epoch 0_81.164%:  Training average Loss: 1.074025\n",
      "Epoch 0_82.232%:  Training average Loss: 1.073397\n",
      "Epoch 0_83.300%:  Training average Loss: 1.073309\n",
      "Epoch 0_84.368%:  Training average Loss: 1.072535\n",
      "Epoch 0_85.436%:  Training average Loss: 1.071983\n",
      "Epoch 0_86.504%:  Training average Loss: 1.071768\n",
      "Epoch 0_87.572%:  Training average Loss: 1.071111\n",
      "Epoch 0_88.640%:  Training average Loss: 1.070659\n",
      "Epoch 0_89.708%:  Training average Loss: 1.071103\n",
      "Epoch 0_90.776%:  Training average Loss: 1.070737\n",
      "Epoch 0_91.844%:  Training average Loss: 1.070471\n",
      "Epoch 0_92.912%:  Training average Loss: 1.070103\n",
      "Epoch 0_93.980%:  Training average Loss: 1.070701\n",
      "Epoch 0_95.048%:  Training average Loss: 1.070470\n",
      "Epoch 0_96.116%:  Training average Loss: 1.070296\n",
      "Epoch 0_97.184%:  Training average Loss: 1.070330\n",
      "Epoch 0_98.252%:  Training average Loss: 1.069855\n",
      "Epoch 0_99.320%:  Training average Loss: 1.069470\n",
      "Epoch 0 :  Verification average Loss: 1.007944, Verification accuracy: 59.414347%,Total Time:38.848037\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model_dict/model_glove does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_accuracy \u001b[38;5;241m<\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m total_data_num:\n\u001b[0;32m     80\u001b[0m     best_accuracy \u001b[38;5;241m=\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m total_data_num\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_dict/model_glove/epoch_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m_accuracy_\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_correct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_data_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is saved in model_dict/model_glove/epoch_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_accuracy_\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;241m%\u001b[39m (i, total_correct \u001b[38;5;241m/\u001b[39m total_data_num)\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\homework_ML\\lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\homework_ML\\lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\homework_ML\\lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory model_dict/model_glove does not exist."
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import time\n",
    "\n",
    "epoch = 100\n",
    "best_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    # 训练\n",
    "    for batch in train_iterator:\n",
    "        steps += 1\n",
    "        # print(steps)\n",
    "        optimizer.zero_grad()  #  梯度缓存清零\n",
    "\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)  # [batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = (\n",
    "            torch.max(out, dim=1)[1].view(batch_label.size())  # get the indices\n",
    "            == batch_label\n",
    "        ).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print(\n",
    "                \"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                % (\n",
    "                    i,\n",
    "                    steps\n",
    "                    * train_iterator.batch_size\n",
    "                    * 100\n",
    "                    / len(train_iterator.dataset),\n",
    "                    total_loss / steps,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # 每个epoch都验证一下\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_data_num = len(dev_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    for batch in dev_iterator:\n",
    "        steps += 1\n",
    "        batch_text = batch.Phrase\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        correct = (\n",
    "            torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label\n",
    "        ).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        print(\n",
    "            \"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "            % (\n",
    "                i,\n",
    "                total_loss / steps,\n",
    "                total_correct * 100 / total_data_num,\n",
    "                time.time() - start_time,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if best_accuracy < total_correct / total_data_num:\n",
    "            best_accuracy = total_correct / total_data_num\n",
    "            torch.save(\n",
    "                model,\n",
    "                \"model_dict/model_glove/epoch_%d_accuracy_%f\"\n",
    "                % (i, total_correct / total_data_num),\n",
    "            )\n",
    "            print(\n",
    "                \"Model is saved in model_dict/model_glove/epoch_%d_accuracy_%f\"\n",
    "                % (i, total_correct / total_data_num)\n",
    "            )\n",
    "            # torch.cuda.empty_cache()\n",
    "    break  # 运行时去除break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试-重新读取文件（方便重写成.py文件）\n",
    "PATH = \"model_dict/model_glove/epoch_0_accuracy_0.586647\"\n",
    "model = torch.load(PATH)\n",
    "\n",
    "total_loss = 0.0\n",
    "accuracy = 0.0\n",
    "total_correct = 0.0\n",
    "total_data_num = len(train_iterator.dataset)\n",
    "steps = 0.0\n",
    "start_time = time.time()\n",
    "for batch in test_iterator:\n",
    "    steps += 1\n",
    "    batch_text = batch.Phrase\n",
    "    batch_label = batch.Sentiment\n",
    "    out = model(batch_text)\n",
    "    loss = criterion(out, batch_label)\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "    total_correct = total_correct + correct.item()\n",
    "    # break\n",
    "\n",
    "print(\n",
    "    \"Test average Loss: %f, Test accuracy: %f，Total time: %f\"\n",
    "    % (total_loss / steps, total_correct / total_data_num, time.time() - start_time)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
