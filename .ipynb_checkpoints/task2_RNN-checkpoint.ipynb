{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n实验目的：实现基于RNN的文本分类\\n\\n实验内容：\\n1）词嵌入初始化方式：随机embedding、加载glove\\n2）CNN/RNN的特征抽取\\n3）Dropout\\n\\n\\n参考：\\nhttps://arxiv.org/abs/1408.5882\\nhttps://github.com/yokusama/CNN_Sentence_Classification\\nhttps://torchtext.readthedocs.io/en/latest/\\nhttp://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\\nhttps://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\\n\\n\\nhttps://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "实验目的：实现基于RNN的文本分类\n",
    "\n",
    "实验内容：\n",
    "1）词嵌入初始化方式：随机embedding、加载glove\n",
    "2）CNN/RNN的特征抽取\n",
    "3）Dropout\n",
    "\n",
    "\n",
    "参考：\n",
    "https://arxiv.org/abs/1408.5882\n",
    "https://github.com/yokusama/CNN_Sentence_Classification\n",
    "https://torchtext.readthedocs.io/en/latest/\n",
    "http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "\n",
    "https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\nlp_beginer_solution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "print(os.getcwd())\n",
    "\n",
    "dir_all_data='data\\\\task2_all_data.tsv'\n",
    "\n",
    "BATCH_SIZE=10\n",
    "\n",
    "cpu=True   #True   False \n",
    "if cpu :\n",
    "    USE_CUDA = False\n",
    "    DEVICE = torch.device('cpu')\n",
    "else:\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件中读取数据\n",
    "data_all=pd.read_csv(dir_all_data,sep='\\t')\n",
    "#print(all_data.shape)    #(156060, 4)\n",
    "#print(all_data.keys())   #['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']\n",
    "idx =np.arange(data_all.shape[0])\n",
    "#print(data_all.head())\n",
    "#print(type(idx))   #<class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle，划分验证集、测试集,并保存\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "#print(idx)\n",
    "np.random.shuffle(idx)  \n",
    "#print(idx)\n",
    "\n",
    "train_size=int(len(idx) * 0.6)\n",
    "test_size =int(len(idx) * 0.8)\n",
    "\n",
    "data_all.iloc[idx[:train_size], :].to_csv('data/task2_train.csv',index=False)\n",
    "data_all.iloc[idx[train_size:test_size], :].to_csv(\"data/task2_test.csv\", index=False)\n",
    "data_all.iloc[idx[test_size:], :].to_csv(\"data/task2_dev.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torchtext采用声明式方法加载数据\n",
    "from torchtext import data\n",
    "PAD_TOKEN='<pad>'\n",
    "TEXT = data.Field(sequential=True,batch_first=True, lower=True, pad_token=PAD_TOKEN)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, unk_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据\n",
    "\n",
    "datafields = [(\"PhraseId\", None), # 不需要的filed设置为None\n",
    "              (\"SentenceId\", None),\n",
    "              ('Phrase', TEXT),\n",
    "              ('Sentiment', LABEL)]\n",
    "train_data = data.TabularDataset(path='data/task2_train2.csv', format='csv',\n",
    "                                fields=datafields)\n",
    "dev_data  = data.TabularDataset(path='data/task2_dev.csv', format='csv',\n",
    "                                fields=datafields)\n",
    "test_data = data.TabularDataset(path='data/task2_test2.csv', format='csv',\n",
    "                                fields=datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词典，字符映射到embedding\n",
    "TEXT.build_vocab(train_data,  vectors= 'glove.6B.50d',   #可以提前下载好\n",
    "                 unk_init= lambda x:torch.nn.init.uniform_(x, a=-0.25, b=0.25))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#得到索引，PAD_TOKEN='<pad>'\n",
    "PAD_INDEX = TEXT.vocab.stoi[PAD_TOKEN]\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到词向量\n",
    "#pretrained_embeddings=TEXT.vocab.vectors\n",
    "#print(type(TEXT.vocab.vectors))\n",
    "#print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建迭代器\n",
    "train_iterator = data.BucketIterator(train_data, batch_size=BATCH_SIZE, \n",
    "                                     train=True, shuffle=True,device=DEVICE)\n",
    "\n",
    "dev_iterator = data.Iterator(dev_data, batch_size=len(dev_data), train=False,\n",
    "                         sort=False, device=DEVICE)\n",
    "\n",
    "test_iterator = data.Iterator(test_data, batch_size=len(test_data), train=False,\n",
    "                          sort=False, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16525 6\n"
     ]
    }
   ],
   "source": [
    "#部分参数设置\n",
    "embedding_choice='glove'   #  'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "embedding_dim =50\n",
    "dropout_p=0.5\n",
    "\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "print(vocab_size,label_num)\n",
    "\n",
    "hidden_size=50  #隐藏单元数\n",
    "num_layers=2  #层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_choice=embedding_choice        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        \n",
    "        if self.embedding_choice==  'rand':\n",
    "            self.embedding=nn.Embedding(num_embeddings,embedding_dim)\n",
    "        if self.embedding_choice==  'glove':\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim, \n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "        #input_size (输入的特征维度),hidden_size ,num_layers \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
    "                            batch_first=True,dropout=dropout_p,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)    \n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num)  # 2 for bidirection\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):      # (Batch_size, Length) \n",
    "        # Set initial hidden and cell states \n",
    "        # h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        # c_n (num_layers * num_directions, batch, hidden_size): \n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            h0=h0.cuda()\n",
    "            c0=c0.cuda()\n",
    "        \n",
    "        x=self.embedding(x)     #(Batch_size, Length) \n",
    "                                       #(Batch_size,  Length, Dimention) \n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))   #(Batch_size, Length，Dimention) \n",
    "                                        # (batch_size, Length, hidden_size)  \n",
    "        out=self.dropout(out)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])   # (batch_size, Length, hidden_size)  \n",
    "                           # (batch_size, label_num)  \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型\n",
    "\n",
    "model=LSTM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()   #损失函数\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_0.801%:  Training average Loss: 1.440362, Training accuracy: 0.003564\n",
      "Epoch 0_1.602%:  Training average Loss: 1.365597, Training accuracy: 0.007761\n",
      "Epoch 0_2.403%:  Training average Loss: 1.331405, Training accuracy: 0.011846\n",
      "Epoch 0_3.204%:  Training average Loss: 1.323612, Training accuracy: 0.015747\n",
      "Epoch 0_4.005%:  Training average Loss: 1.295128, Training accuracy: 0.019960\n",
      "Epoch 0_4.806%:  Training average Loss: 1.280781, Training accuracy: 0.024005\n",
      "Epoch 0_5.607%:  Training average Loss: 1.272381, Training accuracy: 0.028146\n",
      "Epoch 0_6.408%:  Training average Loss: 1.261118, Training accuracy: 0.032495\n",
      "Epoch 0_7.209%:  Training average Loss: 1.261520, Training accuracy: 0.036284\n",
      "Epoch 0_8.010%:  Training average Loss: 1.255846, Training accuracy: 0.040489\n",
      "Epoch 0_8.811%:  Training average Loss: 1.254589, Training accuracy: 0.044494\n",
      "Epoch 0_9.612%:  Training average Loss: 1.255967, Training accuracy: 0.048338\n",
      "Epoch 0_10.413%:  Training average Loss: 1.251414, Training accuracy: 0.052303\n",
      "Epoch 0_11.214%:  Training average Loss: 1.246942, Training accuracy: 0.056476\n",
      "Epoch 0_12.015%:  Training average Loss: 1.243201, Training accuracy: 0.060825\n",
      "Epoch 0_12.815%:  Training average Loss: 1.241981, Training accuracy: 0.064942\n",
      "Epoch 0_13.616%:  Training average Loss: 1.241061, Training accuracy: 0.068795\n",
      "Epoch 0_14.417%:  Training average Loss: 1.240171, Training accuracy: 0.072920\n",
      "Epoch 0_15.218%:  Training average Loss: 1.239245, Training accuracy: 0.076997\n",
      "Epoch 0_16.019%:  Training average Loss: 1.236006, Training accuracy: 0.081250\n",
      "Epoch 0_16.820%:  Training average Loss: 1.234245, Training accuracy: 0.085375\n",
      "Epoch 0_17.621%:  Training average Loss: 1.234242, Training accuracy: 0.089516\n",
      "Epoch 0_18.422%:  Training average Loss: 1.231593, Training accuracy: 0.093577\n",
      "Epoch 0_19.223%:  Training average Loss: 1.231291, Training accuracy: 0.097574\n",
      "Epoch 0_20.024%:  Training average Loss: 1.229451, Training accuracy: 0.101843\n",
      "Epoch 0_20.825%:  Training average Loss: 1.228688, Training accuracy: 0.105744\n",
      "Epoch 0_21.626%:  Training average Loss: 1.227999, Training accuracy: 0.109701\n",
      "Epoch 0_22.427%:  Training average Loss: 1.226793, Training accuracy: 0.113834\n",
      "Epoch 0_23.228%:  Training average Loss: 1.226366, Training accuracy: 0.117902\n",
      "Epoch 0_24.029%:  Training average Loss: 1.225040, Training accuracy: 0.122172\n",
      "Epoch 0_24.830%:  Training average Loss: 1.224703, Training accuracy: 0.126152\n",
      "Epoch 0_25.631%:  Training average Loss: 1.224344, Training accuracy: 0.130349\n",
      "Epoch 0_26.432%:  Training average Loss: 1.222990, Training accuracy: 0.134466\n",
      "Epoch 0_27.233%:  Training average Loss: 1.220512, Training accuracy: 0.138784\n",
      "Epoch 0_28.034%:  Training average Loss: 1.219013, Training accuracy: 0.143077\n",
      "Epoch 0_28.835%:  Training average Loss: 1.216383, Training accuracy: 0.147602\n",
      "Epoch 0_29.636%:  Training average Loss: 1.215063, Training accuracy: 0.151831\n",
      "Epoch 0_30.437%:  Training average Loss: 1.214185, Training accuracy: 0.156028\n",
      "Epoch 0_31.238%:  Training average Loss: 1.212993, Training accuracy: 0.160242\n",
      "Epoch 0_32.039%:  Training average Loss: 1.211509, Training accuracy: 0.164623\n",
      "Epoch 0_32.840%:  Training average Loss: 1.210608, Training accuracy: 0.168748\n",
      "Epoch 0_33.641%:  Training average Loss: 1.208767, Training accuracy: 0.173145\n",
      "Epoch 0_34.442%:  Training average Loss: 1.206957, Training accuracy: 0.177526\n",
      "Epoch 0_35.243%:  Training average Loss: 1.205372, Training accuracy: 0.182020\n",
      "Epoch 0_36.044%:  Training average Loss: 1.203874, Training accuracy: 0.186345\n",
      "Epoch 0_36.845%:  Training average Loss: 1.201213, Training accuracy: 0.190798\n",
      "Epoch 0_37.645%:  Training average Loss: 1.199968, Training accuracy: 0.195004\n",
      "Epoch 0_38.446%:  Training average Loss: 1.197897, Training accuracy: 0.199353\n",
      "Epoch 0_39.247%:  Training average Loss: 1.196885, Training accuracy: 0.203486\n",
      "Epoch 0_40.048%:  Training average Loss: 1.194699, Training accuracy: 0.208115\n",
      "Epoch 0_40.849%:  Training average Loss: 1.192189, Training accuracy: 0.212641\n",
      "Epoch 0_41.650%:  Training average Loss: 1.189595, Training accuracy: 0.217270\n",
      "Epoch 0_42.451%:  Training average Loss: 1.187675, Training accuracy: 0.221900\n",
      "Epoch 0_43.252%:  Training average Loss: 1.185883, Training accuracy: 0.226498\n",
      "Epoch 0_44.053%:  Training average Loss: 1.184659, Training accuracy: 0.230831\n",
      "Epoch 0_44.854%:  Training average Loss: 1.183170, Training accuracy: 0.235156\n",
      "Epoch 0_45.655%:  Training average Loss: 1.182897, Training accuracy: 0.239641\n",
      "Epoch 0_46.456%:  Training average Loss: 1.181485, Training accuracy: 0.244167\n",
      "Epoch 0_47.257%:  Training average Loss: 1.180207, Training accuracy: 0.248612\n",
      "Epoch 0_48.058%:  Training average Loss: 1.178335, Training accuracy: 0.253218\n",
      "Epoch 0_48.859%:  Training average Loss: 1.176672, Training accuracy: 0.257799\n",
      "Epoch 0_49.660%:  Training average Loss: 1.174410, Training accuracy: 0.262557\n",
      "Epoch 0_50.461%:  Training average Loss: 1.172952, Training accuracy: 0.267035\n",
      "Epoch 0_51.262%:  Training average Loss: 1.171880, Training accuracy: 0.271336\n",
      "Epoch 0_52.063%:  Training average Loss: 1.169870, Training accuracy: 0.275941\n",
      "Epoch 0_52.864%:  Training average Loss: 1.167930, Training accuracy: 0.280603\n",
      "Epoch 0_53.665%:  Training average Loss: 1.166098, Training accuracy: 0.285321\n",
      "Epoch 0_54.466%:  Training average Loss: 1.164075, Training accuracy: 0.289926\n",
      "Epoch 0_55.267%:  Training average Loss: 1.162708, Training accuracy: 0.294484\n",
      "Epoch 0_56.068%:  Training average Loss: 1.161092, Training accuracy: 0.299009\n",
      "Epoch 0_56.869%:  Training average Loss: 1.159381, Training accuracy: 0.303671\n",
      "Epoch 0_57.670%:  Training average Loss: 1.158307, Training accuracy: 0.308164\n",
      "Epoch 0_58.471%:  Training average Loss: 1.156564, Training accuracy: 0.312730\n",
      "Epoch 0_59.272%:  Training average Loss: 1.155078, Training accuracy: 0.317255\n",
      "Epoch 0_60.073%:  Training average Loss: 1.153562, Training accuracy: 0.321821\n",
      "Epoch 0_60.874%:  Training average Loss: 1.152365, Training accuracy: 0.326418\n",
      "Epoch 0_61.675%:  Training average Loss: 1.151021, Training accuracy: 0.330944\n",
      "Epoch 0_62.475%:  Training average Loss: 1.149653, Training accuracy: 0.335573\n",
      "Epoch 0_63.276%:  Training average Loss: 1.148546, Training accuracy: 0.340131\n",
      "Epoch 0_64.077%:  Training average Loss: 1.146565, Training accuracy: 0.345049\n",
      "Epoch 0_64.878%:  Training average Loss: 1.145344, Training accuracy: 0.349542\n",
      "Epoch 0_65.679%:  Training average Loss: 1.144509, Training accuracy: 0.354004\n",
      "Epoch 0_66.480%:  Training average Loss: 1.143224, Training accuracy: 0.358577\n",
      "Epoch 0_67.281%:  Training average Loss: 1.142004, Training accuracy: 0.363071\n",
      "Epoch 0_68.082%:  Training average Loss: 1.140971, Training accuracy: 0.367628\n",
      "Epoch 0_68.883%:  Training average Loss: 1.139981, Training accuracy: 0.372194\n",
      "Epoch 0_69.684%:  Training average Loss: 1.138831, Training accuracy: 0.376775\n",
      "Epoch 0_70.485%:  Training average Loss: 1.137665, Training accuracy: 0.381229\n",
      "Epoch 0_71.286%:  Training average Loss: 1.136918, Training accuracy: 0.385746\n",
      "Epoch 0_72.087%:  Training average Loss: 1.135917, Training accuracy: 0.390344\n",
      "Epoch 0_72.888%:  Training average Loss: 1.134639, Training accuracy: 0.395029\n",
      "Epoch 0_73.689%:  Training average Loss: 1.133019, Training accuracy: 0.399819\n",
      "Epoch 0_74.490%:  Training average Loss: 1.131648, Training accuracy: 0.404497\n",
      "Epoch 0_75.291%:  Training average Loss: 1.130574, Training accuracy: 0.409030\n",
      "Epoch 0_76.092%:  Training average Loss: 1.129947, Training accuracy: 0.413443\n",
      "Epoch 0_76.893%:  Training average Loss: 1.128907, Training accuracy: 0.418073\n",
      "Epoch 0_77.694%:  Training average Loss: 1.128182, Training accuracy: 0.422518\n",
      "Epoch 0_78.495%:  Training average Loss: 1.127445, Training accuracy: 0.426860\n",
      "Epoch 0_79.296%:  Training average Loss: 1.126632, Training accuracy: 0.431449\n",
      "Epoch 0_80.097%:  Training average Loss: 1.125018, Training accuracy: 0.436223\n",
      "Epoch 0_80.898%:  Training average Loss: 1.124241, Training accuracy: 0.440804\n",
      "Epoch 0_81.699%:  Training average Loss: 1.123278, Training accuracy: 0.445434\n",
      "Epoch 0_82.500%:  Training average Loss: 1.122716, Training accuracy: 0.449839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_83.301%:  Training average Loss: 1.122194, Training accuracy: 0.454213\n",
      "Epoch 0_84.102%:  Training average Loss: 1.121309, Training accuracy: 0.458722\n",
      "Epoch 0_84.903%:  Training average Loss: 1.120379, Training accuracy: 0.463408\n",
      "Epoch 0_85.704%:  Training average Loss: 1.119544, Training accuracy: 0.468069\n",
      "Epoch 0_86.504%:  Training average Loss: 1.118981, Training accuracy: 0.472603\n",
      "Epoch 0_87.305%:  Training average Loss: 1.117888, Training accuracy: 0.477433\n",
      "Epoch 0_88.106%:  Training average Loss: 1.117178, Training accuracy: 0.481942\n",
      "Epoch 0_88.907%:  Training average Loss: 1.116469, Training accuracy: 0.486564\n",
      "Epoch 0_89.708%:  Training average Loss: 1.116262, Training accuracy: 0.491057\n",
      "Epoch 0_90.509%:  Training average Loss: 1.115885, Training accuracy: 0.495479\n",
      "Epoch 0_91.310%:  Training average Loss: 1.115274, Training accuracy: 0.500076\n",
      "Epoch 0_92.111%:  Training average Loss: 1.114289, Training accuracy: 0.504626\n",
      "Epoch 0_92.912%:  Training average Loss: 1.113708, Training accuracy: 0.509151\n",
      "Epoch 0_93.713%:  Training average Loss: 1.112498, Training accuracy: 0.513949\n",
      "Epoch 0_94.514%:  Training average Loss: 1.111704, Training accuracy: 0.518691\n",
      "Epoch 0_95.315%:  Training average Loss: 1.111184, Training accuracy: 0.523360\n",
      "Epoch 0_96.116%:  Training average Loss: 1.110283, Training accuracy: 0.528046\n",
      "Epoch 0_96.917%:  Training average Loss: 1.109955, Training accuracy: 0.532547\n",
      "Epoch 0_97.718%:  Training average Loss: 1.109090, Training accuracy: 0.537321\n",
      "Epoch 0_98.519%:  Training average Loss: 1.108425, Training accuracy: 0.542191\n",
      "Epoch 0_99.320%:  Training average Loss: 1.107648, Training accuracy: 0.546805\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2208243918b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mbatch_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mbatch_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-49499159bfde>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m                                        \u001b[1;31m#(Batch_size,  Length, Dimention)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#(Batch_size, Length，Dimention)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                                         \u001b[1;31m# (batch_size, Length, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 522\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epoch=100\n",
    "best_accuracy=0.0\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        #print(steps)\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "        \n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)    #[batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "                   .view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f, Training accuracy: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps, total_correct/total_data_num))  \n",
    "\n",
    "    #测试\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(dev_iterator.dataset)\n",
    "    steps = 0.0    \n",
    "    for batch in dev_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "        \n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))  \n",
    "        \n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num \n",
    "            torch.save(model,'model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            #torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "PATH='model_dict/model_lstm/epoch_5_a'\n",
    "model = torch.load(PATH)\n",
    "\n",
    "total_loss=0.0\n",
    "accuracy=0.0\n",
    "total_correct=0.0\n",
    "total_data_num = len(train_iterator.dataset)\n",
    "steps = 0.0    \n",
    "start_time=time.time()\n",
    "for batch in test_iterator:\n",
    "    steps+=1\n",
    "    batch_text=batch.Phrase\n",
    "    batch_label=batch.Sentiment\n",
    "    out=model(batch_text)\n",
    "    loss = criterion(out, batch_label)\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "    total_correct = total_correct + correct.item()\n",
    "\n",
    "print(\"Test average Loss: %f, Test accuracy: %f，Total time: %f\"\n",
    "  %(total_loss/steps, total_correct/total_data_num,time.time()-start_time) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epoch=100\n",
    "best_accuracy=0.0\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    if i==1:\n",
    "        break\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    j=0\n",
    "    for batch in train_iterator:\n",
    "        j+=1\n",
    "        if j==3:\n",
    "            break\n",
    "        steps+=1\n",
    "        #print(steps)\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "        \n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        print(batch_label)\n",
    "        #print(batch_text.size())\n",
    "        #print(batch_text)\n",
    "        #print(model.embedding(batch_text))\n",
    "        \n",
    "        \n",
    "\n",
    "#         batch_label=batch.Sentiment\n",
    "        \n",
    "#         out=model(batch_text)    #[batch_size, label_num]\n",
    "#         loss = criterion(out, batch_label)\n",
    "#         total_loss = total_loss + loss.item() \n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()        \n",
    "\n",
    "#         correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "#                    .view(batch_label.size()) == batch_label).sum()\n",
    "#         total_correct = total_correct + correct.item()\n",
    "\n",
    "#         if steps%100==0:\n",
    "#             print(\"Epoch %d_%.3f%%:  Training average Loss: %f, Training accuracy: %f\"\n",
    "#                       %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps, total_correct/total_data_num))  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
